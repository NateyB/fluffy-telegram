>> Downloading cifar-10-binary.tar.gz 100.0%
Successfully downloaded cifar-10-binary.tar.gz 170052171 bytes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-03 13:00:10.149348: step 0, loss = 4.68 (72.9 examples/sec; 1.755 sec/batch)
2017-05-03 13:00:18.016712: step 10, loss = 4.60 (162.7 examples/sec; 0.787 sec/batch)
2017-05-03 13:00:25.879354: step 20, loss = 4.71 (162.8 examples/sec; 0.786 sec/batch)
2017-05-03 13:00:33.673628: step 30, loss = 4.48 (164.2 examples/sec; 0.779 sec/batch)
2017-05-03 13:00:41.403956: step 40, loss = 4.31 (165.6 examples/sec; 0.773 sec/batch)
2017-05-03 13:00:49.224410: step 50, loss = 4.30 (163.7 examples/sec; 0.782 sec/batch)
2017-05-03 13:00:56.963102: step 60, loss = 4.29 (165.4 examples/sec; 0.774 sec/batch)
2017-05-03 13:01:04.705993: step 70, loss = 4.39 (165.3 examples/sec; 0.774 sec/batch)
2017-05-03 13:01:12.489147: step 80, loss = 4.12 (164.5 examples/sec; 0.778 sec/batch)
2017-05-03 13:01:20.331456: step 90, loss = 3.97 (163.2 examples/sec; 0.784 sec/batch)
2017-05-03 13:01:28.294038: step 100, loss = 4.17 (160.8 examples/sec; 0.796 sec/batch)
2017-05-03 13:01:36.068276: step 110, loss = 4.00 (164.6 examples/sec; 0.777 sec/batch)
2017-05-03 13:01:43.857018: step 120, loss = 4.10 (164.3 examples/sec; 0.779 sec/batch)
2017-05-03 13:01:51.647389: step 130, loss = 3.94 (164.3 examples/sec; 0.779 sec/batch)
2017-05-03 13:01:59.400174: step 140, loss = 3.96 (165.1 examples/sec; 0.775 sec/batch)
2017-05-03 13:02:07.188738: step 150, loss = 3.88 (164.3 examples/sec; 0.779 sec/batch)
2017-05-03 13:02:15.116026: step 160, loss = 3.71 (161.5 examples/sec; 0.793 sec/batch)
2017-05-03 13:02:23.559513: step 170, loss = 3.92 (151.6 examples/sec; 0.844 sec/batch)
2017-05-03 13:02:32.934415: step 180, loss = 3.84 (136.5 examples/sec; 0.937 sec/batch)
2017-05-03 13:02:42.738843: step 190, loss = 3.87 (130.6 examples/sec; 0.980 sec/batch)
2017-05-03 13:02:55.357985: step 200, loss = 4.04 (101.4 examples/sec; 1.262 sec/batch)
2017-05-03 13:03:03.163501: step 210, loss = 3.76 (164.0 examples/sec; 0.781 sec/batch)
2017-05-03 13:03:12.005746: step 220, loss = 3.94 (144.8 examples/sec; 0.884 sec/batch)
2017-05-03 13:03:20.530069: step 230, loss = 3.69 (150.2 examples/sec; 0.852 sec/batch)
2017-05-03 13:03:29.683476: step 240, loss = 3.69 (139.8 examples/sec; 0.915 sec/batch)
2017-05-03 13:03:38.891130: step 250, loss = 3.74 (139.0 examples/sec; 0.921 sec/batch)
2017-05-03 13:03:48.109207: step 260, loss = 3.72 (138.9 examples/sec; 0.922 sec/batch)
2017-05-03 13:03:56.027132: step 270, loss = 3.77 (161.7 examples/sec; 0.792 sec/batch)
2017-05-03 13:04:04.427107: step 280, loss = 3.71 (152.4 examples/sec; 0.840 sec/batch)
2017-05-03 13:04:12.712710: step 290, loss = 3.49 (154.5 examples/sec; 0.829 sec/batch)
2017-05-03 13:04:20.763682: step 300, loss = 3.60 (159.0 examples/sec; 0.805 sec/batch)
2017-05-03 13:04:28.528096: step 310, loss = 3.63 (164.9 examples/sec; 0.776 sec/batch)
2017-05-03 13:04:36.307863: step 320, loss = 3.54 (164.5 examples/sec; 0.778 sec/batch)
2017-05-03 13:04:44.213497: step 330, loss = 3.41 (161.9 examples/sec; 0.791 sec/batch)
2017-05-03 13:04:52.055125: step 340, loss = 3.42 (163.2 examples/sec; 0.784 sec/batch)
2017-05-03 13:04:59.784506: step 350, loss = 3.55 (165.6 examples/sec; 0.773 sec/batch)
2017-05-03 13:05:07.562600: step 360, loss = 3.32 (164.6 examples/sec; 0.778 sec/batch)
2017-05-03 13:05:15.387686: step 370, loss = 3.29 (163.6 examples/sec; 0.783 sec/batch)
2017-05-03 13:05:23.183521: step 380, loss = 3.50 (164.2 examples/sec; 0.780 sec/batch)
2017-05-03 13:05:31.055730: step 390, loss = 3.49 (162.6 examples/sec; 0.787 sec/batch)
2017-05-03 13:05:39.025737: step 400, loss = 3.30 (160.6 examples/sec; 0.797 sec/batch)
2017-05-03 13:05:46.808200: step 410, loss = 3.22 (164.5 examples/sec; 0.778 sec/batch)
2017-05-03 13:05:54.600482: step 420, loss = 3.26 (164.3 examples/sec; 0.779 sec/batch)
2017-05-03 13:06:02.606913: step 430, loss = 3.40 (159.9 examples/sec; 0.801 sec/batch)
2017-05-03 13:06:10.607686: step 440, loss = 3.26 (160.0 examples/sec; 0.800 sec/batch)
2017-05-03 13:06:18.731821: step 450, loss = 3.10 (157.6 examples/sec; 0.812 sec/batch)
2017-05-03 13:06:26.763407: step 460, loss = 3.29 (159.4 examples/sec; 0.803 sec/batch)
2017-05-03 13:06:34.598533: step 470, loss = 3.09 (163.4 examples/sec; 0.784 sec/batch)
2017-05-03 13:06:42.602618: step 480, loss = 3.17 (159.9 examples/sec; 0.800 sec/batch)
2017-05-03 13:06:50.765255: step 490, loss = 3.42 (156.8 examples/sec; 0.816 sec/batch)
2017-05-03 13:06:58.885527: step 500, loss = 3.31 (157.6 examples/sec; 0.812 sec/batch)
2017-05-03 13:07:06.694268: step 510, loss = 3.16 (163.9 examples/sec; 0.781 sec/batch)
2017-05-03 13:07:16.454014: step 520, loss = 3.14 (131.2 examples/sec; 0.976 sec/batch)
2017-05-03 13:07:26.044969: step 530, loss = 3.28 (133.5 examples/sec; 0.959 sec/batch)
2017-05-03 13:07:34.366730: step 540, loss = 3.09 (153.8 examples/sec; 0.832 sec/batch)
2017-05-03 13:07:42.143068: step 550, loss = 3.28 (164.6 examples/sec; 0.778 sec/batch)
2017-05-03 13:07:50.943800: step 560, loss = 3.16 (145.4 examples/sec; 0.880 sec/batch)
2017-05-03 13:07:58.728977: step 570, loss = 3.02 (164.4 examples/sec; 0.779 sec/batch)
2017-05-03 13:08:07.614383: step 580, loss = 3.25 (144.1 examples/sec; 0.889 sec/batch)
2017-05-03 13:08:16.126814: step 590, loss = 2.93 (150.4 examples/sec; 0.851 sec/batch)
2017-05-03 13:08:25.408603: step 600, loss = 2.97 (137.9 examples/sec; 0.928 sec/batch)
2017-05-03 13:08:34.702284: step 610, loss = 3.03 (137.7 examples/sec; 0.929 sec/batch)
2017-05-03 13:08:43.209720: step 620, loss = 3.07 (150.5 examples/sec; 0.851 sec/batch)
2017-05-03 13:08:50.978975: step 630, loss = 2.96 (164.8 examples/sec; 0.777 sec/batch)
2017-05-03 13:08:58.693412: step 640, loss = 3.03 (165.9 examples/sec; 0.771 sec/batch)
2017-05-03 13:09:07.719892: step 650, loss = 3.58 (141.8 examples/sec; 0.903 sec/batch)
2017-05-03 13:09:16.096684: step 660, loss = 2.93 (152.8 examples/sec; 0.838 sec/batch)
2017-05-03 13:09:23.807999: step 670, loss = 3.14 (166.0 examples/sec; 0.771 sec/batch)
2017-05-03 13:09:31.574648: step 680, loss = 2.79 (164.8 examples/sec; 0.777 sec/batch)
2017-05-03 13:09:39.990451: step 690, loss = 2.81 (152.1 examples/sec; 0.842 sec/batch)
2017-05-03 13:09:48.128001: step 700, loss = 2.89 (157.3 examples/sec; 0.814 sec/batch)
2017-05-03 13:09:57.199737: step 710, loss = 2.77 (141.1 examples/sec; 0.907 sec/batch)
2017-05-03 13:10:06.779302: step 720, loss = 2.83 (133.6 examples/sec; 0.958 sec/batch)
2017-05-03 13:10:19.617793: step 730, loss = 2.86 (99.7 examples/sec; 1.284 sec/batch)
2017-05-03 13:10:29.503362: step 740, loss = 2.89 (129.5 examples/sec; 0.989 sec/batch)
2017-05-03 13:10:38.863244: step 750, loss = 2.86 (136.8 examples/sec; 0.936 sec/batch)
2017-05-03 13:10:47.311602: step 760, loss = 2.86 (151.5 examples/sec; 0.845 sec/batch)
2017-05-03 13:10:55.231378: step 770, loss = 2.64 (161.6 examples/sec; 0.792 sec/batch)
2017-05-03 13:11:03.054067: step 780, loss = 2.63 (163.6 examples/sec; 0.782 sec/batch)
2017-05-03 13:11:10.788000: step 790, loss = 2.86 (165.5 examples/sec; 0.773 sec/batch)
2017-05-03 13:11:18.702315: step 800, loss = 2.66 (161.7 examples/sec; 0.791 sec/batch)
2017-05-03 13:11:26.422409: step 810, loss = 2.65 (165.8 examples/sec; 0.772 sec/batch)
2017-05-03 13:11:34.056188: step 820, loss = 2.58 (167.7 examples/sec; 0.763 sec/batch)
2017-05-03 13:11:41.716120: step 830, loss = 2.66 (167.1 examples/sec; 0.766 sec/batch)
2017-05-03 13:11:49.671061: step 840, loss = 2.54 (160.9 examples/sec; 0.795 sec/batch)
2017-05-03 13:11:58.191023: step 850, loss = 2.60 (150.2 examples/sec; 0.852 sec/batch)
2017-05-03 13:12:06.967934: step 860, loss = 2.61 (145.8 examples/sec; 0.878 sec/batch)
2017-05-03 13:12:16.154263: step 870, loss = 2.77 (139.3 examples/sec; 0.919 sec/batch)
2017-05-03 13:12:25.776394: step 880, loss = 2.69 (133.0 examples/sec; 0.962 sec/batch)
2017-05-03 13:12:36.798795: step 890, loss = 2.51 (116.1 examples/sec; 1.102 sec/batch)
2017-05-03 13:12:47.372937: step 900, loss = 2.67 (121.1 examples/sec; 1.057 sec/batch)
2017-05-03 13:12:56.098718: step 910, loss = 2.54 (146.7 examples/sec; 0.873 sec/batch)
2017-05-03 13:13:04.857980: step 920, loss = 2.47 (146.1 examples/sec; 0.876 sec/batch)
2017-05-03 13:13:13.758532: step 930, loss = 2.58 (143.8 examples/sec; 0.890 sec/batch)
2017-05-03 13:13:22.058520: step 940, loss = 2.55 (154.2 examples/sec; 0.830 sec/batch)
2017-05-03 13:13:32.497853: step 950, loss = 2.62 (122.6 examples/sec; 1.044 sec/batch)
2017-05-03 13:13:41.288615: step 960, loss = 2.56 (145.6 examples/sec; 0.879 sec/batch)
2017-05-03 13:13:50.533521: step 970, loss = 2.47 (138.5 examples/sec; 0.924 sec/batch)
2017-05-03 13:14:00.806296: step 980, loss = 2.40 (124.6 examples/sec; 1.027 sec/batch)
2017-05-03 13:14:10.734926: step 990, loss = 2.33 (128.9 examples/sec; 0.993 sec/batch)
2017-05-03 13:14:20.882432: step 1000, loss = 2.74 (126.1 examples/sec; 1.015 sec/batch)
2017-05-03 13:14:31.286091: step 1010, loss = 2.45 (123.0 examples/sec; 1.040 sec/batch)
2017-05-03 13:14:40.100963: step 1020, loss = 2.35 (145.2 examples/sec; 0.881 sec/batch)
2017-05-03 13:14:48.419098: step 1030, loss = 2.50 (153.9 examples/sec; 0.832 sec/batch)
2017-05-03 13:14:57.204817: step 1040, loss = 2.54 (145.7 examples/sec; 0.879 sec/batch)
2017-05-03 13:15:07.352523: step 1050, loss = 2.41 (126.1 examples/sec; 1.015 sec/batch)
2017-05-03 13:15:16.206669: step 1060, loss = 2.29 (144.6 examples/sec; 0.885 sec/batch)
2017-05-03 13:15:24.951563: step 1070, loss = 2.33 (146.4 examples/sec; 0.874 sec/batch)
2017-05-03 13:15:33.051642: step 1080, loss = 2.21 (158.0 examples/sec; 0.810 sec/batch)
2017-05-03 13:15:41.735525: step 1090, loss = 2.34 (147.4 examples/sec; 0.868 sec/batch)
2017-05-03 13:15:50.817911: step 1100, loss = 2.29 (140.9 examples/sec; 0.908 sec/batch)
2017-05-03 13:15:59.913247: step 1110, loss = 2.55 (140.7 examples/sec; 0.910 sec/batch)
2017-05-03 13:16:08.404623: step 1120, loss = 2.53 (150.7 examples/sec; 0.849 sec/batch)
2017-05-03 13:16:17.495497: step 1130, loss = 2.11 (140.8 examples/sec; 0.909 sec/batch)
2017-05-03 13:16:26.242328: step 1140, loss = 2.37 (146.3 examples/sec; 0.875 sec/batch)
2017-05-03 13:16:35.131784: step 1150, loss = 2.28 (144.0 examples/sec; 0.889 sec/batch)
2017-05-03 13:16:43.892172: step 1160, loss = 2.10 (146.1 examples/sec; 0.876 sec/batch)
2017-05-03 13:16:53.522864: step 1170, loss = 2.39 (132.9 examples/sec; 0.963 sec/batch)
2017-05-03 13:17:03.229036: step 1180, loss = 2.12 (131.9 examples/sec; 0.971 sec/batch)
2017-05-03 13:17:12.769657: step 1190, loss = 2.17 (134.2 examples/sec; 0.954 sec/batch)
2017-05-03 13:17:23.617313: step 1200, loss = 2.18 (118.0 examples/sec; 1.085 sec/batch)
2017-05-03 13:17:33.561282: step 1210, loss = 2.14 (128.7 examples/sec; 0.994 sec/batch)
2017-05-03 13:17:41.516720: step 1220, loss = 2.10 (160.9 examples/sec; 0.796 sec/batch)
2017-05-03 13:17:49.383649: step 1230, loss = 2.10 (162.7 examples/sec; 0.787 sec/batch)
2017-05-03 13:17:57.205596: step 1240, loss = 2.08 (163.6 examples/sec; 0.782 sec/batch)
2017-05-03 13:18:05.685825: step 1250, loss = 2.07 (150.9 examples/sec; 0.848 sec/batch)
2017-05-03 13:18:14.752424: step 1260, loss = 2.09 (141.2 examples/sec; 0.907 sec/batch)
2017-05-03 13:18:24.174876: step 1270, loss = 2.22 (135.8 examples/sec; 0.942 sec/batch)
2017-05-03 13:18:33.206247: step 1280, loss = 2.05 (141.7 examples/sec; 0.903 sec/batch)
2017-05-03 13:18:42.072057: step 1290, loss = 2.17 (144.4 examples/sec; 0.887 sec/batch)
2017-05-03 13:18:51.238160: step 1300, loss = 2.01 (139.6 examples/sec; 0.917 sec/batch)
2017-05-03 13:19:00.154355: step 1310, loss = 2.10 (143.6 examples/sec; 0.892 sec/batch)
2017-05-03 13:19:09.841388: step 1320, loss = 2.23 (132.1 examples/sec; 0.969 sec/batch)
2017-05-03 13:19:19.198807: step 1330, loss = 2.08 (136.8 examples/sec; 0.936 sec/batch)
2017-05-03 13:19:28.234972: step 1340, loss = 1.96 (141.7 examples/sec; 0.904 sec/batch)
2017-05-03 13:19:37.395059: step 1350, loss = 2.58 (139.7 examples/sec; 0.916 sec/batch)
2017-05-03 13:19:47.008804: step 1360, loss = 2.12 (133.1 examples/sec; 0.961 sec/batch)
2017-05-03 13:19:56.302467: step 1370, loss = 1.86 (137.7 examples/sec; 0.929 sec/batch)
2017-05-03 13:20:04.662793: step 1380, loss = 2.10 (153.1 examples/sec; 0.836 sec/batch)
2017-05-03 13:20:16.021311: step 1390, loss = 1.92 (112.7 examples/sec; 1.136 sec/batch)
2017-05-03 13:20:24.789095: step 1400, loss = 1.98 (146.0 examples/sec; 0.877 sec/batch)
2017-05-03 13:20:34.636119: step 1410, loss = 2.05 (130.0 examples/sec; 0.985 sec/batch)
2017-05-03 13:20:43.744422: step 1420, loss = 1.93 (140.5 examples/sec; 0.911 sec/batch)
2017-05-03 13:20:52.738373: step 1430, loss = 2.08 (142.3 examples/sec; 0.899 sec/batch)
2017-05-03 13:21:01.712491: step 1440, loss = 2.02 (142.6 examples/sec; 0.897 sec/batch)
2017-05-03 13:21:11.287411: step 1450, loss = 2.08 (133.7 examples/sec; 0.957 sec/batch)
2017-05-03 13:21:20.119076: step 1460, loss = 2.09 (144.9 examples/sec; 0.883 sec/batch)
2017-05-03 13:21:28.737306: step 1470, loss = 1.99 (148.5 examples/sec; 0.862 sec/batch)
2017-05-03 13:21:37.405936: step 1480, loss = 1.90 (147.7 examples/sec; 0.867 sec/batch)
2017-05-03 13:21:45.948146: step 1490, loss = 2.05 (149.8 examples/sec; 0.854 sec/batch)
2017-05-03 13:21:54.576905: step 1500, loss = 2.08 (148.3 examples/sec; 0.863 sec/batch)
2017-05-03 13:22:03.076807: step 1510, loss = 1.88 (150.6 examples/sec; 0.850 sec/batch)
2017-05-03 13:22:11.798011: step 1520, loss = 1.71 (146.8 examples/sec; 0.872 sec/batch)
2017-05-03 13:22:19.998168: step 1530, loss = 2.10 (156.1 examples/sec; 0.820 sec/batch)
2017-05-03 13:22:29.415187: step 1540, loss = 1.92 (135.9 examples/sec; 0.942 sec/batch)
2017-05-03 13:22:37.791046: step 1550, loss = 1.92 (152.8 examples/sec; 0.838 sec/batch)
2017-05-03 13:22:47.837561: step 1560, loss = 1.97 (127.4 examples/sec; 1.005 sec/batch)
2017-05-03 13:22:57.159290: step 1570, loss = 1.98 (137.3 examples/sec; 0.932 sec/batch)
2017-05-03 13:23:05.550548: step 1580, loss = 1.95 (152.5 examples/sec; 0.839 sec/batch)
2017-05-03 13:23:14.261213: step 1590, loss = 1.63 (146.9 examples/sec; 0.871 sec/batch)
2017-05-03 13:23:23.983087: step 1600, loss = 1.98 (131.7 examples/sec; 0.972 sec/batch)
2017-05-03 13:23:33.768924: step 1610, loss = 1.80 (130.8 examples/sec; 0.979 sec/batch)
2017-05-03 13:23:42.523807: step 1620, loss = 1.86 (146.2 examples/sec; 0.875 sec/batch)
2017-05-03 13:23:51.210291: step 1630, loss = 1.85 (147.4 examples/sec; 0.869 sec/batch)
2017-05-03 13:24:00.357080: step 1640, loss = 1.92 (139.9 examples/sec; 0.915 sec/batch)
2017-05-03 13:24:08.597343: step 1650, loss = 1.81 (155.3 examples/sec; 0.824 sec/batch)
2017-05-03 13:24:17.194805: step 1660, loss = 1.82 (148.9 examples/sec; 0.860 sec/batch)
2017-05-03 13:24:26.332535: step 1670, loss = 1.72 (140.1 examples/sec; 0.914 sec/batch)
2017-05-03 13:24:34.996844: step 1680, loss = 1.83 (147.7 examples/sec; 0.866 sec/batch)
2017-05-03 13:24:45.172124: step 1690, loss = 1.81 (125.8 examples/sec; 1.018 sec/batch)
2017-05-03 13:24:55.986994: step 1700, loss = 1.88 (118.4 examples/sec; 1.081 sec/batch)
2017-05-03 13:25:05.865298: step 1710, loss = 1.96 (129.6 examples/sec; 0.988 sec/batch)
2017-05-03 13:25:15.230670: step 1720, loss = 2.04 (136.7 examples/sec; 0.937 sec/batch)
2017-05-03 13:25:23.452010: step 1730, loss = 1.74 (155.7 examples/sec; 0.822 sec/batch)
2017-05-03 13:25:32.262542: step 1740, loss = 1.73 (145.3 examples/sec; 0.881 sec/batch)
2017-05-03 13:25:39.933580: step 1750, loss = 1.70 (166.9 examples/sec; 0.767 sec/batch)
2017-05-03 13:25:48.719233: step 1760, loss = 1.91 (145.7 examples/sec; 0.879 sec/batch)
2017-05-03 13:25:56.593851: step 1770, loss = 1.66 (162.5 examples/sec; 0.787 sec/batch)
2017-05-03 13:26:04.362836: step 1780, loss = 1.81 (164.8 examples/sec; 0.777 sec/batch)
2017-05-03 13:26:12.994988: step 1790, loss = 1.64 (148.3 examples/sec; 0.863 sec/batch)
2017-05-03 13:26:22.536290: step 1800, loss = 1.70 (134.2 examples/sec; 0.954 sec/batch)
2017-05-03 13:26:31.887462: step 1810, loss = 1.74 (136.9 examples/sec; 0.935 sec/batch)
2017-05-03 13:26:40.569849: step 1820, loss = 1.75 (147.4 examples/sec; 0.868 sec/batch)
2017-05-03 13:26:50.584339: step 1830, loss = 1.83 (127.8 examples/sec; 1.001 sec/batch)
2017-05-03 13:27:00.392111: step 1840, loss = 1.69 (130.5 examples/sec; 0.981 sec/batch)
2017-05-03 13:27:10.335008: step 1850, loss = 1.76 (128.7 examples/sec; 0.994 sec/batch)
2017-05-03 13:27:19.557153: step 1860, loss = 1.82 (138.8 examples/sec; 0.922 sec/batch)
2017-05-03 13:27:27.987941: step 1870, loss = 1.70 (151.8 examples/sec; 0.843 sec/batch)
2017-05-03 13:27:36.707927: step 1880, loss = 1.71 (146.8 examples/sec; 0.872 sec/batch)
2017-05-03 13:27:45.435272: step 1890, loss = 1.61 (146.7 examples/sec; 0.873 sec/batch)
2017-05-03 13:27:55.023679: step 1900, loss = 1.83 (133.5 examples/sec; 0.959 sec/batch)
2017-05-03 13:28:03.946648: step 1910, loss = 1.74 (143.4 examples/sec; 0.892 sec/batch)
2017-05-03 13:28:13.192882: step 1920, loss = 2.01 (138.4 examples/sec; 0.925 sec/batch)
2017-05-03 13:28:23.187274: step 1930, loss = 1.69 (128.1 examples/sec; 0.999 sec/batch)
2017-05-03 13:28:32.144264: step 1940, loss = 1.69 (142.9 examples/sec; 0.896 sec/batch)
2017-05-03 13:28:40.193683: step 1950, loss = 1.76 (159.0 examples/sec; 0.805 sec/batch)
2017-05-03 13:28:49.089478: step 1960, loss = 1.58 (143.9 examples/sec; 0.890 sec/batch)
2017-05-03 13:28:58.749252: step 1970, loss = 1.81 (132.5 examples/sec; 0.966 sec/batch)
2017-05-03 13:29:08.496969: step 1980, loss = 1.49 (131.3 examples/sec; 0.975 sec/batch)
2017-05-03 13:29:20.125206: step 1990, loss = 1.47 (110.1 examples/sec; 1.163 sec/batch)
2017-05-03 13:29:28.919464: step 2000, loss = 1.72 (145.5 examples/sec; 0.879 sec/batch)

# Output
2017-05-03 13:30:09.362615: precision @ 1 = 0.658

